House Price Prediction using Deep Learning (PyTorch) 

‚ñ¢ Overview :-


This project implements a deep learning‚Äìbased regression model using PyTorch to predict house prices based on multiple property-related features. The goal of the project is to 
explore how neural networks perform on structured/tabular data and to analyze training behavior, validation performance, and overfitting trends.The model is trained and evaluated on
a real-world Indian house price dataset.


‚ñ¢ Problem Statement :-


‚Ä¢ Accurate house price prediction is an important problem in real estate analytics. Given a set of features describing a property (such as location-related attributes, size, and 
  amenities), the task is to predict the price of the house as a continuous value.


‚ñ¢ This project focuses on :-


‚Ä¢ Framing house price prediction as a regression problem
‚Ä¢ Applying a feedforward neural network
‚Ä¢ Monitoring loss, validation loss, and MAE during training


‚ñ¢ Dataset :-


‚Ä¢  The dataset used in this project is not included in the repository due to size constraints.

üìå Dataset Source:
https://www.kaggle.com/datasets/mohamedafsal007/house-price-dataset-of-india

‚Ä¢ Dataset Description :-

The dataset contains house listings from India
Each sample includes 16 numerical features
Target variable represents the house price
Features (high-level)
The dataset includes features related to:
Property size and layout
Location-related attributes
Availability of amenities
Other numerical indicators affecting house prices

*Note: The dataset is preprocessed and split into training, validation, and test sets within the code.*


‚ñ¢ Approach


‚Ä¢ Model Architecture :-

Fully connected feedforward neural network
Implemented using PyTorch
Designed for regression on tabular data

‚Ä¢ Training Strategy

Loss Function: Mean Squared Error (MSE)
Evaluation Metric: Mean Absolute Error (MAE)
Optimizer: Adam
Training performed on a local machine (GPU)

‚Ä¢ Data Split

The dataset is split as follows (as observed during runtime):
Training set: 9356 samples
Validation set: 2340 samples
Test set: 2924 samples
Each sample contains 16 input features.

‚Ä¢ Training Results

Below are selected training logs showing loss, validation loss, and MAE progression:

Epoch 0:
Train Loss: 0.418
Val Loss: 0.408
MAE: 0.778

Epoch 300:
Train Loss: 0.140
Val Loss: 0.145
MAE: 0.432

Epoch 600:
Train Loss: 0.125
Val Loss: 0.141
MAE: 0.425

Epoch 900:
Train Loss: 0.108
Val Loss: 0.144
MAE: 0.429

Epoch 1800:
Train Loss: 0.071
Val Loss: 0.170
MAE: 0.468


‚ñ¢ Observations


‚Ä¢ Training loss consistently decreases with more epochs
‚Ä¢ Validation loss improves initially but starts increasing after a point
‚Ä¢ MAE shows a U-shaped trend, indicating overfitting
‚Ä¢ Best performance is achieved around 500‚Äì700 epochs

‚Ä¢ This behavior highlights the importance of:

Early stopping
Regularization
Careful epoch selection


‚ñ¢ Project Structure :-


House-price-prediction-pytorch/
‚îÇ
‚îú‚îÄ‚îÄ model.py # Model definition, training loop, and evaluation
‚îú‚îÄ‚îÄ prediction.py # Model prediction file 
‚îú‚îÄ‚îÄ defaults.py # Default values for each feature  
‚îú‚îÄ‚îÄ README.md # Project documentation
‚îú‚îÄ‚îÄ requirements.txt # Python dependencies

For simplicity and experimentation, preprocessing, training, and evaluation logic are kept in a single script.

How to Run:-

1Ô∏è‚É£ Install dependencies

pip install -r requirements.txt

2Ô∏è‚É£ Download dataset

Download the dataset from the Kaggle link provided above
Place it in the expected path used in model.py

3Ô∏è‚É£ Train the model

python model.py


‚ñ¢ Limitations :-


Neural networks may not outperform classical ML models on tabular data without extensive tuning
Model shows signs of overfitting after long training
No advanced regularization techniques (dropout, batch norm) are used
Dataset size and feature quality limit generalization


‚ñ¢ Learnings :-


Practical experience using PyTorch for regression
Understanding loss vs validation loss behavior
Observing overfitting in deep learning models
Importance of evaluation metrics like MAE for regression tasks


‚ñ¢ Future Improvements :-


Add early stopping
Compare with traditional ML models (Linear Regression, XGBoost)
Feature engineering and normalization improvements
Hyperparameter tuning


‚ñ¢ Technologies Used :-


Python
PyTorch
NumPy
Pandas
Scikit-learn
